# ðŸ“ˆ NOW: Linear Regression â€” The First Supervised Algorithm

---

## ðŸ”¢ What is Linear Regression?

- **Type**: Supervised Learning â†’ Regression (predicts continuous number)
- **Goal**: Find the best-fit line (or hyperplane) that minimizes error between predicted and actual values.
- **Assumption**: Relationship between features and target is approximately linear.

---

## ðŸ§  Mathematical Form

For `n` features: Å· = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b

Or in vector form: Å· = wáµ€x + b

    - `Å·` = predicted value
    - `w` = weights (learned from data)
    - `b` = bias / intercept
    - `x` = input features

> ðŸ“Œ This is the same as the **hyperplane equation** â€” but here, weâ€™re not separating classes, weâ€™re predicting values.

---

## ðŸ“‰ How It Learns: Cost Function & Optimization

- **Cost Function**: Mean Squared Error (MSE)
  MSE = (1/m) \* Î£(Å·áµ¢ - yáµ¢)Â²

- **Optimization**: Gradient Descent (or Normal Equation) â†’ finds `w` and `b` that minimize MSE.

---

## âœ… When to Use Linear Regression

- Predicting house prices, sales, temperature, salary
- Baseline model for any regression problem
- Interpretable â€” you can see how each feature affects output

---

## âš ï¸ Limitations

- Assumes linear relationship
- Sensitive to outliers
- Can underfit if data is complex

> ðŸ’¡ Fix with: Polynomial features, regularization (Ridge/Lasso), or upgrade to Random Forest / XGBoost.

---

ðŸ“Œ **Summary Flow**:
Equation of Line â†’ Plane â†’ Hyperplane
â†“
Distance from Plane â†’ Used in SVM
â†“
Instance vs Model Based â†’ Linear Regression is Model-Based
â†“
Å· = wáµ€x + b â†’ Learned via minimizing MSE
