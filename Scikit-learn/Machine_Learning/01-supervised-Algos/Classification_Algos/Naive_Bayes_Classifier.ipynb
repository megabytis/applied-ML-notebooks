{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4c44d8",
   "metadata": {},
   "source": [
    "# üìä Naive Bayes Classifier\n",
    "\n",
    "## üåü Real-Life Example: The Weather & Golf Decision\n",
    "\n",
    "Imagine you're **Tom**, and every morning you decide whether to play golf based on the weather. You've kept a diary for 14 days:\n",
    "\n",
    "| Day | Outlook   | Temperature | Humidity | Windy | Play Golf? |\n",
    "|-----|-----------|-------------|----------|-------|------------|\n",
    "| 1   | Rainy     | Hot         | High     | No    | ‚ùå No      |\n",
    "| 2   | Rainy     | Hot         | High     | Yes   | ‚ùå No      |\n",
    "| 3   | Overcast  | Hot         | High     | No    | ‚úÖ Yes     |\n",
    "| 4   | Sunny     | Mild        | High     | No    | ‚úÖ Yes     |\n",
    "| 5   | Sunny     | Cool        | Normal   | No    | ‚úÖ Yes     |\n",
    "| 6   | Sunny     | Cool        | Normal   | Yes   | ‚ùå No      |\n",
    "| 7   | Overcast  | Cool        | Normal   | Yes   | ‚úÖ Yes     |\n",
    "| 8   | Rainy     | Mild        | High     | No    | ‚ùå No      |\n",
    "| 9   | Rainy     | Cool        | Normal   | No    | ‚úÖ Yes     |\n",
    "| 10  | Sunny     | Mild        | Normal   | No    | ‚úÖ Yes     |\n",
    "| 11  | Rainy     | Mild        | Normal   | Yes   | ‚úÖ Yes     |\n",
    "| 12  | Overcast  | Mild        | High     | Yes   | ‚úÖ Yes     |\n",
    "| 13  | Overcast  | Hot         | Normal   | No    | ‚úÖ Yes     |\n",
    "| 14  | Sunny     | Mild        | High     | Yes   | ‚ùå No      |\n",
    "\n",
    "**Today's weather**: Sunny, Hot, Normal humidity, No wind  \n",
    "**Question**: Should Tom play golf today?\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Step 1: What Would a Smart Person Do?\n",
    "\n",
    "A smart person would look at their diary and ask:\n",
    "- \"On **sunny** days, how often did I play golf?\"\n",
    "- \"When it was **hot**, how often did I play?\"\n",
    "- \"With **normal humidity**, what happened?\"\n",
    "- \"When it was **not windy**, what was the outcome?\"\n",
    "\n",
    "Then they'd combine all this information to make a decision.\n",
    "\n",
    "**This is exactly what Naive Bayes does!**\n",
    "\n",
    "---\n",
    "## üìê Bayes' Theorem\n",
    "\n",
    "### Definition of Conditional Probability:\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(B|A) = \\frac{P(A \\cap B)}{P(A)}\n",
    "$$\n",
    "\n",
    "### Bayes' Theorem:\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)}\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $P(A|B)$: Probability of A given B (posterior)\n",
    "- $P(B|A)$: Probability of B given A (likelihood)  \n",
    "- $P(A)$: Prior probability of A (prior)\n",
    "- $P(B)$: Total probability of B (evidence)\n",
    "- $P(A \\cap B)$: Probability of both A and B occurring together\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Step 2: Let's Count From the Diary\n",
    "\n",
    "### Overall Statistics:\n",
    "- **Total days**: 14\n",
    "- **Played golf (Yes)**: 9 days ‚Üí P(Yes) = 9/14 ‚âà 0.64\n",
    "- **Didn't play (No)**: 5 days ‚Üí P(No) = 5/14 ‚âà 0.36\n",
    "\n",
    "### For \"Sunny\" Outlook:\n",
    "- **Sunny days**: 5 total (Days 4, 5, 6, 10, 14)\n",
    "- **Sunny + Played**: 3 days (Days 4, 5, 10) ‚Üí P(Sunny | Yes) = 3/9 = 0.33\n",
    "   - golf played days in sunny days / total golf played days\n",
    "   - i.e. probability of sunny days given that golf is played .\n",
    "\n",
    "- **Sunny + Didn't Play**: 2 days (Days 6, 14) ‚Üí P(Sunny | No) = 2/5 = 0.40\n",
    "   - probability of sunny days given that golf is not played\n",
    "\n",
    "### For \"Hot\" Temperature:\n",
    "- **Hot days**: 4 total (Days 1, 2, 3, 13)\n",
    "- **Hot + Played**: 2 days (Days 3, 13) ‚Üí P(Hot | Yes) = 2/9 ‚âà 0.22\n",
    "- **Hot + Didn't Play**: 2 days (Days 1, 2) ‚Üí P(Hot | No) = 2/5 = 0.40\n",
    "\n",
    "### For \"Normal\" Humidity:\n",
    "- **Normal humidity days**: 7 total\n",
    "- **Normal + Played**: 6 days ‚Üí P(Normal | Yes) = 6/9 ‚âà 0.67\n",
    "- **Normal + Didn't Play**: 1 day ‚Üí P(Normal | No) = 1/5 = 0.20\n",
    "\n",
    "### For \"No Wind\":\n",
    "- **No wind days**: 8 total\n",
    "- **No wind + Played**: 6 days ‚Üí P(No Wind | Yes) = 6/9 ‚âà 0.67\n",
    "- **No wind + Didn't Play**: 2 days ‚Üí P(No Wind | No) = 2/5 = 0.40\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Step 3: The \"Naive\" Assumption\n",
    "\n",
    "Here's the key insight: **Naive Bayes assumes each weather condition is independent**.\n",
    "\n",
    "This means:\n",
    "- Whether it's sunny doesn't affect whether it's hot\n",
    "- Humidity doesn't depend on wind\n",
    "- Each condition gives separate evidence\n",
    "\n",
    "**In reality, this isn't true** (sunny days are often hot), but it makes the math simple and still works well!\n",
    "\n",
    "So instead of calculating P(Sunny AND Hot AND Normal AND No Wind | Yes), we calculate:\n",
    "\n",
    "P(Sunny | Yes) √ó P(Hot | Yes) √ó P(Normal | Yes) √ó P(No Wind | Yes)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Step 4: Calculate the Scores\n",
    "\n",
    "### Score for \"Play Golf = YES\":\n",
    "\n",
    "P(Yes) √ó P(Sunny | Yes) √ó P(Hot | Yes) √ó P(Normal | Yes) √ó P(No Wind | Yes)\n",
    "\n",
    "= 0.64 √ó 0.33 √ó 0.22 √ó 0.67 √ó 0.67\n",
    "\n",
    "‚âà 0.64 √ó 0.033\n",
    "\n",
    "‚âà 0.021\n",
    "\n",
    "\n",
    "### Score for \"Play Golf = NO\":\n",
    "\n",
    "P(No) √ó P(Sunny | No) √ó P(Hot | No) √ó P(Normal | No) √ó P(No Wind | No)\n",
    "\n",
    "= 0.36 √ó 0.40 √ó 0.40 √ó 0.20 √ó 0.40\n",
    "\n",
    "= 0.36 √ó 0.0128\n",
    "\n",
    "‚âà 0.0046\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Step 5: Make the Decision\n",
    "\n",
    "- **YES score**: 0.021\n",
    "- **NO score**: 0.0046\n",
    "\n",
    "Since **0.021 > 0.0046**, Naive Bayes predicts: **‚úÖ PLAY GOLF!**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Connecting to Technical Terms\n",
    "\n",
    "### What We Just Did = Naive Bayes Algorithm\n",
    "\n",
    "1. **Bayes' Theorem**: \n",
    "   - We calculated P(Class | Features) using P(Features | Class) √ó P(Class)\n",
    "   - This is the core of Bayes' theorem\n",
    "\n",
    "2. **\"Naive\" Assumption**: \n",
    "   - We assumed P(Feature1 AND Feature2 | Class) = P(Feature1 | Class) √ó P(Feature2 | Class)\n",
    "   - This independence assumption is why it's called \"naive\"\n",
    "\n",
    "3. **Prior Probability**: \n",
    "   - P(Yes) = 0.64 and P(No) = 0.36 are called \"priors\"\n",
    "   - They represent our initial belief before seeing today's weather\n",
    "\n",
    "4. **Likelihood**: \n",
    "   - P(Sunny | Yes) = 0.33 is the \"likelihood\"\n",
    "   - It tells us how likely sunny weather is given that we played golf\n",
    "\n",
    "5. **Posterior Probability**: \n",
    "   - The final scores (0.021 and 0.0046) are proportional to \"posterior probabilities\"\n",
    "   - We choose the class with the highest posterior\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How This Works for Any Problem\n",
    "\n",
    "### General Formula:\n",
    "For any new example with features [F1, F2, F3, ..., Fn]:\n",
    "\n",
    "Score for Class C = P(C) √ó P(F1 | C) √ó P(F2 | C) √ó ... √ó P(Fn | C)\n",
    "\n",
    "\n",
    "\n",
    "### For Different Data Types:\n",
    "\n",
    "#### **Text Classification (Spam Detection)**:\n",
    "- Features = words in email\n",
    "- P(\"FREE\" | Spam) = how often \"FREE\" appears in spam emails\n",
    "- P(\"meeting\" | Ham) = how often \"meeting\" appears in legitimate emails\n",
    "\n",
    "#### **Medical Diagnosis**:\n",
    "- Features = symptoms (fever, cough, headache)\n",
    "- P(Fever | Flu) = how often fever occurs with flu\n",
    "- P(Cough | Common Cold) = how often cough occurs with common cold\n",
    "\n",
    "#### **Product Reviews**:\n",
    "- Features = words in review\n",
    "- P(\"excellent\" | Positive) = how often \"excellent\" appears in positive reviews\n",
    "- P(\"terrible\" | Negative) = how often \"terrible\" appears in negative reviews\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è The Zero Probability Problem\n",
    "\n",
    "### What if a word never appeared?\n",
    "Imagine in your golf diary, you never had a \"Sunny + Hot + Normal + No Wind\" combination.\n",
    "\n",
    "**Problem**: If any P(Feature | Class) = 0, the entire score becomes 0!\n",
    "\n",
    "### Solution: Laplace Smoothing\n",
    "Instead of counting raw frequencies, we add 1 to every count:\n",
    "\n",
    "**Original**: P(Sunny | Yes) = 3/9  \n",
    "**With smoothing**: P(Sunny | Yes) = (3+1)/(9+3) = 4/12 = 0.33\n",
    "\n",
    "Where \"3\" is the number of possible outlook values (Sunny, Rainy, Overcast).\n",
    "\n",
    "This ensures no probability is ever zero!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why Naive Bayes Works So Well\n",
    "\n",
    "### The Secret Sauce:\n",
    "1. **It doesn't need perfect independence** ‚Äì even if features are somewhat related, the relative scores still work\n",
    "2. **It focuses on the biggest patterns** ‚Äì small errors in individual probabilities cancel out\n",
    "3. **It's incredibly fast** ‚Äì just counting and multiplying\n",
    "4. **It works with small data** ‚Äì you don't need millions of examples\n",
    "\n",
    "### When It's Perfect:\n",
    "- **Text classification**: Words in documents are somewhat independent\n",
    "- **Real-time decisions**: Email spam filtering, chatbot responses\n",
    "- **Baseline models**: Quick first attempt before complex models\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Key Takeaways\n",
    "\n",
    "- **Naive Bayes = Smart counting + Simple math**\n",
    "- **\"Naive\" = assumes features are independent** (simplification that works)\n",
    "- **Works by calculating scores for each class** and picking the highest\n",
    "- **Perfect for text problems** like spam detection and sentiment analysis\n",
    "- **Handles the \"never seen before\" problem** with Laplace smoothing\n",
    "- **Fast, simple, and surprisingly accurate**\n",
    "\n",
    "> **Remember**: Naive Bayes is like your friend who makes decisions by looking at past patterns and saying \"Based on what I've seen before, this is most likely what will happen!\" üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e791542",
   "metadata": {},
   "source": [
    "### Types of Naive Bayes\n",
    "| Type | Use-case | Data Type |\n",
    "|------|-----------|-----------|\n",
    "| GaussianNB | Continuous | Real numbers |\n",
    "| MultinomialNB | Discrete counts | Word frequency |\n",
    "| BernoulliNB | Binary | Word presence (0/1) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71412cc",
   "metadata": {},
   "source": [
    "# 1. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1ae05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c817f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_wine()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df[\"Target\"] = data.target\n",
    "\n",
    "X = df.drop(\"Target\", axis=1)\n",
    "y = df[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ea8db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a4a792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d9e8609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cnfusion Matrix:\n",
      " [[14  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0  8]]\n",
      "\n",
      "Accuracy Score: 1.000\n",
      "\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Cnfusion Matrix:\\n {cm}\")\n",
    "print(f\"\\nAccuracy Score: {acc:.3f}\")\n",
    "print(f\"\\nClassification Report: {cr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e194db2",
   "metadata": {},
   "source": [
    "# 2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21156b56",
   "metadata": {},
   "source": [
    "building a SPAM classifier using multinomial Naive Bayes. Exactly how Gmail or Whatsapp spam filters start out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad9ca0",
   "metadata": {},
   "source": [
    "### üß† Step 1: Real-world idea\n",
    "\n",
    "We want to predict whether a message is spam or not spam.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Message                        | Label          |\n",
    "| ------------------------------ | -------------- |\n",
    "| \"Win money now!!!\"             | spam           |\n",
    "| \"Your project meeting at 10am\" | ham (not spam) |\n",
    "| \"Free laptop just for you\"     | spam           |\n",
    "| \"Can we talk later?\"           | ham            |\n",
    "\n",
    "\n",
    "### üß© Step 2: Concept\n",
    "\n",
    "The algorithm looks for which words are common in spam vs ham messages.\n",
    "\n",
    "It calculates probabilities like:\n",
    "- P(word = ‚Äúwin‚Äù | spam)\n",
    "- P(word = ‚Äúwin‚Äù | ham)\n",
    "\n",
    "Then, when a new message comes in ‚Äî say \"Win a free phone now\" ‚Äî\n",
    "it multiplies those probabilities for all words and picks whichever class (spam/ham) gives a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba606dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "597376c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messeges = [\n",
    "    \"Win money now\",\n",
    "    \"Free cash prize\",\n",
    "    \"Click to claim your reward\",\n",
    "    \"Earn extra income today\",\n",
    "    \"Meeting at 10am\",\n",
    "    \"Let's have lunch tomorrow\",\n",
    "    \"Project submission due\",\n",
    "    \"Are you free tonight?\",\n",
    "]\n",
    "\n",
    "labels = [\"spam\", \"spam\", \"spam\", \"spam\", \"ham\", \"ham\", \"ham\", \"ham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e3972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nWHAT HAPPENED HERE ?\\n\\n#############################################\\nPHASE-1 : fit() phase ‚Üí VOCABULARY GENERATION\\n#############################################\\n\\n1. CountVectorizer.fit() scans all messages \\n2. Splits each message into individual words (tokenization)\\n3. Collects all unique words across all messages\\n4. sorts them alphabetically\\n5. stores thi svocabulary internally in the vectozier object\\n\\n- Now vectorizer.vocabulary_ contains all the word-to-index mapping :\\n\\n{'win': 25, 'money': 15, 'now': 16, 'free': 9, 'cash': 3, 'prize': 17, 'click': 5, 'to': 21, 'claim': 4, 'your': 27, 'reward': 19, 'earn': 7, 'extra': 8, 'income': 11, 'today': 22, 'meeting': 14, 'at': 2, '10am': 0, 'let': 12, 'have': 10, 'lunch': 13, 'tomorrow': 23, 'project': 18, 'submission': 20, 'due': 6, 'are': 1, 'you': 26, 'tonight': 24}\\n\\n#############################################################\\nPHASE-2 : transform() phase ‚Üí DOCUMENT-TERM MATRIX GENERATION\\n#############################################################\\n\\n1. CountVectorizer.transform() uses the vocabulary created in phase-1\\n2. for each message, counts how many times each word appears\\n3. creates the numerical matrix (sparse matrix format)\\n\\n- Now X conatains our document-term matrix\\n\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(messeges)\n",
    "\n",
    "\"\"\" \n",
    "WHAT HAPPENED HERE ?\n",
    "\n",
    "#############################################\n",
    "PHASE-1 : fit() phase ‚Üí VOCABULARY GENERATION\n",
    "#############################################\n",
    "\n",
    "1. CountVectorizer.fit() scans all messages \n",
    "2. Splits each message into individual words (tokenization)\n",
    "3. Collects all unique words across all messages\n",
    "4. sorts them alphabetically\n",
    "5. stores thi svocabulary internally in the vectozier object\n",
    "\n",
    "- Now vectorizer.vocabulary_ contains all the word-to-index mapping :\n",
    "\n",
    "{'win': 25, 'money': 15, 'now': 16, 'free': 9, 'cash': 3, 'prize': 17, 'click': 5, 'to': 21, 'claim': 4, 'your': 27, 'reward': 19, 'earn': 7, 'extra': 8, 'income': 11, 'today': 22, 'meeting': 14, 'at': 2, '10am': 0, 'let': 12, 'have': 10, 'lunch': 13, 'tomorrow': 23, 'project': 18, 'submission': 20, 'due': 6, 'are': 1, 'you': 26, 'tonight': 24}\n",
    "\n",
    "#############################################################\n",
    "PHASE-2 : transform() phase ‚Üí DOCUMENT-TERM MATRIX GENERATION\n",
    "#############################################################\n",
    "\n",
    "1. CountVectorizer.transform() uses the vocabulary created in phase-1\n",
    "2. for each message, counts how many times each word appears\n",
    "3. creates the numerical matrix (sparse matrix format)\n",
    "\n",
    "- Now X conatains our document-term matrix\n",
    "\n",
    "üëá click the below link to see How document-term matrix Looks like üëá\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d2916",
   "metadata": {},
   "source": [
    "click here üëâ [Document-term Matrix](./assets/document_Matrix.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7e1e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15790377",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fa3f4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cnfusion Matrix:\n",
      " [[1 0]\n",
      " [1 0]]\n",
      "\n",
      "Accuracy Score: 0.500\n",
      "\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.50      1.00      0.67         1\n",
      "        spam       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/megabytis/boooooom/python/myenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/megabytis/boooooom/python/myenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/megabytis/boooooom/python/myenv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Cnfusion Matrix:\\n {cm}\")\n",
    "print(f\"\\nAccuracy Score: {acc:.3f}\")\n",
    "print(f\"\\nClassification Report: {cr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97cae8",
   "metadata": {},
   "source": [
    "#### Testing our model :--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63724be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions: ['spam' 'ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "sample = [\n",
    "    \"Win a free laptop now\",\n",
    "    \"Let's schedule a meeting\",\n",
    "    \"Click here to grab your free cash prize\",\n",
    "]\n",
    "\n",
    "sample_vector = vectorizer.transform(sample)\n",
    "print(f\"\\nPredictions: {model.predict(sample_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42960b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
