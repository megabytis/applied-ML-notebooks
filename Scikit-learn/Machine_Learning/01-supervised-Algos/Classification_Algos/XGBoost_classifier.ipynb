{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7638e0",
   "metadata": {},
   "source": [
    "# üöÄ XGBoost: Extreme Gradient Boosting ‚Äî Complete Guide\n",
    "\n",
    "We already know:\n",
    "\n",
    "**Decision Tree**: A model that splits data based on feature values (like ‚Äúif Income > 50k ‚Üí yes‚Äù).\n",
    "\n",
    "**Random Forest**: Builds many trees in parallel and averages their results.\n",
    "\n",
    "Now‚Ä¶\n",
    "\n",
    "üëâ XGBoost builds many trees sequentially ‚Äî\n",
    "each new tree tries to fix the mistakes made by the previous ones.\n",
    "That‚Äôs the boosting part.\n",
    "\n",
    "## üß† What is XGBoost?\n",
    "\n",
    "### Core Idea\n",
    "XGBoost = **Extreme Gradient Boosting**  \n",
    "- Builds **many decision trees sequentially**  \n",
    "- Each new tree **tries to fix the mistakes** made by all previous trees  \n",
    "- Final prediction = **combined wisdom** of all trees\n",
    "\n",
    "### Analogy: The Mentor-Student Learning Loop\n",
    "Imagine you're learning to shoot basketball:\n",
    "\n",
    "1. **First attempt**: You shoot ‚Üí miss badly  \n",
    "2. **Mentor observes**: \"You're shooting too low\"  \n",
    "3. **Second attempt**: You adjust ‚Üí still miss, but better  \n",
    "4. **Mentor again**: \"Now you're overcompensating\"  \n",
    "5. **Keep going**: Each attempt fixes the last mistake  \n",
    "6. **Final result**: After many corrections, you become accurate!\n",
    "\n",
    "**XGBoost works exactly like this** ‚Äî each tree is a \"correction attempt\" guided by the errors of previous trees.\n",
    "\n",
    "---\n",
    "\n",
    "## üå≥ How XGBoost Works: Step-by-Step\n",
    "\n",
    "### 1. **Start with a Weak Guess**\n",
    "- Model makes an initial prediction for all examples  \n",
    "- Often very simple (e.g., predicts everyone gets denied a loan)  \n",
    "- **Compute errors**: Differences between predicted and actual values\n",
    "\n",
    "### 2. **Build First Tree to Correct Mistakes**\n",
    "- Focus **only on examples where prediction was wrong**  \n",
    "- Tree learns: \"When features look like THIS, I should adjust prediction by THAT much\"  \n",
    "- **Update predictions**: Add small corrections toward correct answers\n",
    "\n",
    "### 3. **Evaluate Residuals / Gradient**\n",
    "- Calculate **how wrong** predictions are and **in which direction** they need correction  \n",
    "- This **gradient** mathematically guides the next tree  \n",
    "- Think: \"Mentor gives precise feedback: 'Add 0.3 to your prediction'\"\n",
    "\n",
    "### 4. **Add a New Tree**\n",
    "- Another tree is trained **specifically to fix remaining errors**  \n",
    "- Predictions are updated incrementally  \n",
    "- **Learning rate** controls how much each tree contributes (like how much you listen to mentor)\n",
    "\n",
    "### 5. **Repeat Sequentially**\n",
    "- Continue building trees until:\n",
    "  - `n_estimators` is reached, OR  \n",
    "  - Error stops improving significantly  \n",
    "- Each tree only sees **mistakes that previous trees couldn‚Äôt fix fully**\n",
    "\n",
    "### 6. **Final Prediction**\n",
    "- Combine contributions of all trees (weighted by learning rate)  \n",
    "- Result is a **strong ensemble predictor** that learned from its own mistakes\n",
    "\n",
    "\n",
    "<img src=\"images/XG-Boost.webp\">\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Key Parameters Explained\n",
    "\n",
    "| Parameter | What It Controls | Analogy |\n",
    "|-----------|------------------|---------|\n",
    "| `n_estimators` | Number of sequential trees (rounds of learning) | How many times you ask your mentor for feedback |\n",
    "| `learning_rate` | Step size for each tree (smaller = slower but more stable) | How much you listen to each piece of advice (conservative vs bold) |\n",
    "| `max_depth` | Maximum depth of each decision tree | How detailed each correction is (simple tip vs complex strategy) |\n",
    "| `subsample` | Fraction of data per tree (prevents overfitting) | Mentor only watches some of your shots to avoid overfitting to specific attempts |\n",
    "| `colsample_bytree` | Fraction of features used per tree | Mentor focuses on different aspects each time (form, angle, power) |\n",
    "| `gamma` | Minimum improvement needed to split a node | Mentor only gives advice if it's worth the effort |\n",
    "| `reg_alpha` / `reg_lambda` | L1/L2 regularization strength | Mentor discourages overly complex corrections |\n",
    "| `scale_pos_weight` | Handles class imbalance | Mentor pays extra attention to rare mistakes (like missed free throws) |\n",
    "\n",
    "---\n",
    "\n",
    "## üÜö Random Forest vs XGBoost\n",
    "\n",
    "| Concept | Random Forest | XGBoost |\n",
    "|---------|---------------|---------|\n",
    "| **Tree Growth** | Parallel (all trees built independently) | Sequential (each tree learns from previous mistakes) |\n",
    "| **Goal** | Reduce variance (stability) | Reduce bias **and** variance (accuracy + stability) |\n",
    "| **Weighting** | Equal vote for all trees | Later trees correct earlier mistakes |\n",
    "| **Performance** | Good baseline, robust | Often stronger, more accurate, competition winner |\n",
    "| **Learning Style** | \"Average multiple opinions\" | \"Build smarter opinions sequentially, learning from past mistakes\" |\n",
    "\n",
    "### Analogy Comparison:\n",
    "- **Random Forest**: Ask 100 friends independently what they think ‚Üí take majority vote  \n",
    "- **XGBoost**: Ask friend #1 ‚Üí they're wrong ‚Üí ask friend #2 to fix friend #1's mistake ‚Üí ask friend #3 to fix remaining errors ‚Üí and so on\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why XGBoost is So Powerful\n",
    "\n",
    "### ‚úÖ Key Strengths:\n",
    "1. **Sequential Learning**: Each tree builds on previous knowledge\n",
    "2. **Built-in Regularization**: Prevents overfitting through multiple mechanisms\n",
    "3. **Handles Missing Values**: Automatically learns best direction for missing data\n",
    "4. **Feature Importance**: Shows which features drive decisions\n",
    "5. **Optimized Performance**: Extremely fast and memory efficient\n",
    "6. **Flexible Objectives**: Works for classification, regression, ranking\n",
    "\n",
    "### üö® When to Be Careful:\n",
    "- **Parameter Sensitivity**: Needs proper tuning for best results\n",
    "- **Sequential Training**: Slower to train than Random Forest (but faster prediction)\n",
    "- **Black Box Nature**: Harder to interpret than single decision trees\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Practical Considerations\n",
    "\n",
    "### Data Preparation:\n",
    "- **No scaling needed**: XGBoost works with raw feature values\n",
    "- **Categorical features**: One-hot encode or use native categorical support\n",
    "- **Missing values**: Handled automatically (no imputation required)\n",
    "\n",
    "### Model Interpretation:\n",
    "- **Feature importance**: Shows which features matter most\n",
    "- **Partial dependence plots**: Understand feature effects\n",
    "- **Tree visualization**: Inspect individual trees for debugging\n",
    "\n",
    "### Advanced Techniques:\n",
    "- **Early stopping**: Stop training when validation performance plateaus\n",
    "- **Cross-validation**: Built-in support for robust evaluation\n",
    "- **Custom objectives**: Define your own loss functions for specialized problems\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaways\n",
    "\n",
    "- XGBoost = **Sequential ensemble learning** that corrects its own mistakes\n",
    "- **Each tree is weak alone**, but the **ensemble becomes extremely strong**\n",
    "- **Regularization is built-in** through multiple parameters\n",
    "- **Often the best choice** for structured/tabular data problems\n",
    "- **Requires parameter tuning** but rewards effort with superior performance\n",
    "\n",
    "> **Remember**: XGBoost isn't just many trees ‚Äî it's a **learning system** that gets smarter by analyzing and fixing its own errors, just like a human learner with a good mentor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c22189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99599141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"assets/loan_approval_data.csv\")\n",
    "\n",
    "# One-hot encoding categorical features (if any)\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "X = df.drop(\"Approved\", axis=1)\n",
    "y = df[\"Approved\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa582fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c675175",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c6166ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:31:55] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b285f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.65\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.70      0.65        94\n",
      "           1       0.70      0.60      0.65       106\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.65      0.65      0.65       200\n",
      "weighted avg       0.66      0.65      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy Score: \", acc)\n",
    "print(\"\\nClassification Report: \\n\", cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
