{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05322260",
   "metadata": {},
   "source": [
    "# Feature Scaling: \n",
    "\n",
    "## What is Feature Scaling?\n",
    "\n",
    "Imagine we’re comparing two things:\n",
    "\n",
    "1. The height of a person (in meters): e.g., 1.75, 1.80, 1.65.\n",
    "2. The weight of a person (in kilograms): e.g., 70, 80, 90.\n",
    "\n",
    "Now, height values are much smaller than weight values. If we give these numbers to a computer, it might think weight is more important because the numbers are bigger. But in reality, both height and weight are equally important!\n",
    "\n",
    "To fix this, we \"scale\" the features so that all the numbers are on the same level. This process is called **Feature Scaling**.\n",
    "\n",
    "\n",
    "So Technically;\n",
    "> Feature scaling is the process of transforming numerical features so that they are on the same scale. This helps machine learning algorithms work better by avoiding bias toward features with larger values.\n",
    "\n",
    "\n",
    "## Why Do We Need Feature Scaling?\n",
    "\n",
    "1. Avoid Bias: Features with larger values can dominate the model, even if they’re not more important.\n",
    "\n",
    "2. Faster learning: Algorithms like Gradient Descent converge faster when features are scaled.\n",
    "\n",
    "3. Better results: Scaling helps the model treat all features fairly.\n",
    "\n",
    "### Affected Algorithms (i.e. ML algos which work better with Scaling) : \n",
    "  - KNN, K-Means, SVM, Logistic/Linear Regression, PCA, Neural Nets\n",
    "### Non Affected Algorithms :\n",
    "  - Tree-based models (Decision Tree, Random Forest, XGBoost)\n",
    "\n",
    "## Types of Feature Scaling\n",
    "1. **Standardization (Z-Score Normalization)**:\n",
    "   - Converts data to have a mean of 0 and a standard deviation of 1.\n",
    "   - Formula:  \n",
    "     $$\n",
    "     z = \\frac{x - \\text{mean}}{\\text{standard deviation}}\n",
    "     $$\n",
    "   - Use case: When data has outliers or follows a normal distribution.\n",
    "\n",
    "2. **Normalization (Min-Max Scaling)**:\n",
    "   - Scales data to a fixed range (usually 0 to 1).\n",
    "   - Formula:  \n",
    "     $$\n",
    "     x_{\\text{scaled}} = \\frac{x - \\text{min}}{\\text{max} - \\text{min}}\n",
    "     $$\n",
    "   - Use case: When you need all features in the same range (e.g., image data).\n",
    "\n",
    "## Tools for Feature Scaling\n",
    "Scikit-learn provides two main tools:\n",
    "- `StandardScaler`: For standardization.\n",
    "- `MinMaxScaler`: For normalization.\n",
    "\n",
    "## Example Workflow\n",
    "1. Create your dataset.\n",
    "2. Apply a scaler (`StandardScaler` or `MinMaxScaler`).\n",
    "3. Transform the data and convert it back to a DataFrame for readability.\n",
    "\n",
    "## Key Takeaways\n",
    "- Standardization works well with outliers and normal data.\n",
    "- Normalization is useful when you need a fixed range (e.g., 0 to 1).\n",
    "- Always scale your features before training machine learning models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f830d5e5-3839-48cc-96d8-f9fdbf950442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d87f21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Height(cm)  Weight(kg)\n",
      "0         150          50\n",
      "1         160          60\n",
      "2         170          70\n",
      "3         180          80\n",
      "4         190          90\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "data = {\"Height(cm)\": [150, 160, 170, 180, 190], \"Weight(kg)\": [50, 60, 70, 80, 90]}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7b85b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Standard Scaler --------------\n",
    "std_scaler = StandardScaler()\n",
    "df[\"Height_std\"] = std_scaler.fit_transform(df[[\"Height(cm)\"]])\n",
    "df[\"Weight_std\"] = std_scaler.fit_transform(df[[\"Weight(kg)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55498484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Min-Max Scaler ------------\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df[\"Height_mm\"] = minmax_scaler.fit_transform(df[[\"Height(cm)\"]])\n",
    "df[\"Weight_mm\"] = minmax_scaler.fit_transform(df[[\"Weight(kg)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e70a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled Data:\n",
      "   Height(cm)  Weight(kg)  Height_std  Weight_std  Height_mm  Weight_mm\n",
      "0         150          50   -1.414214   -1.414214       0.00       0.00\n",
      "1         160          60   -0.707107   -0.707107       0.25       0.25\n",
      "2         170          70    0.000000    0.000000       0.50       0.50\n",
      "3         180          80    0.707107    0.707107       0.75       0.75\n",
      "4         190          90    1.414214    1.414214       1.00       1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nScaled Data:\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
