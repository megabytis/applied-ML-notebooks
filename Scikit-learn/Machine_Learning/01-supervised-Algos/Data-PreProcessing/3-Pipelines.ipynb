{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dadbae",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Pipelines in Machine Learning\n",
    "\n",
    "## üß† What‚Äôs the Problem?\n",
    "\n",
    "So far, every time we trained a model, we had to do this manually:\n",
    "\n",
    "1. Encode categorical data  \n",
    "2. Scale numerical features  \n",
    "3. Split data  \n",
    "4. Train model  \n",
    "5. Predict  \n",
    "\n",
    "But in real workflows:\n",
    "- **We don‚Äôt wanna repeat all these steps** for every dataset.\n",
    "- **Risk of data leakage**: For example, accidentally using test data in `.fit()` during scaling.\n",
    "\n",
    "That‚Äôs where **Pipelines** come in.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° What is a Pipeline?\n",
    "\n",
    "A **Pipeline** in `sklearn` is a tool that chains multiple steps together into a single workflow. Each step in the pipeline performs a specific task, such as preprocessing or modeling.\n",
    "\n",
    "Think of it as a **conveyor belt** ‚Äî raw data goes in, predictions come out.\n",
    "\n",
    "The key idea is:\n",
    "\n",
    "- We pass our raw data into the pipeline.\n",
    "- The pipeline applies each step sequentially (e.g., scaling ‚Üí training a model).\n",
    "- The final output is the result of the last step (e.g., predictions).\n",
    "\n",
    "This ensures that all steps are applied consistently and avoids mistakes like data leakage\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Why Use Pipelines?\n",
    "\n",
    "1. **Automation**: Automate repetitive tasks like encoding, scaling, and training.\n",
    "2. **Avoid Data Leakage**: Ensures that preprocessing (e.g., scaling) only uses training data.\n",
    "3. **Clean Code**: Keeps your code organized and reproducible.\n",
    "4. **Easy to Deploy**: Simplifies the process of deploying models into production.\n",
    "\n",
    "---\n",
    "\n",
    "## üåü How Does It Work?\n",
    "\n",
    "A Pipeline consists of multiple steps, each defined as a tuple `(name, transformer/model)`:\n",
    "- **Transformers**: Handle preprocessing steps like encoding or scaling.\n",
    "- **Model**: The final step is always an estimator (e.g., LogisticRegression).\n",
    "\n",
    "Example Workflow:\n",
    "1. **Step 1**: Encode categorical variables.\n",
    "2. **Step 2**: Scale numerical features.\n",
    "3. **Step 3**: Train a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85ddc0",
   "metadata": {},
   "source": [
    "## Without Pipelines üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3feb2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import  load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0ecbdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba542d",
   "metadata": {},
   "source": [
    "## With Pipelines üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02a5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Creating pipeline\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"model\", LogisticRegression())])\n",
    "\n",
    "'''\n",
    "- here, Pipeline([...]) creates a new pipeline object &  inside the Pipeline, we pass a list of tuples, where each tuple represents a step in the workflow.\n",
    "\n",
    "- In First Tuple: (\"scaler\", StandardScaler()) ;\n",
    "- \"scaler\": This is the name of the first step in the pipeline. We can name it anything we want (e.g., \"scaling_step\", \"preprocessing\", etc.), but it should be descriptive.\n",
    "- StandardScaler(): This is the actual transformer for the first step. It scales the features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "- In Second Tuple: (\"model\", LogisticRegression()) ;\n",
    "- \"model\": This is the name of the second step in the pipeline. Again, we can name it anything (e.g., \"classifier\", \"logistic_regression\", etc.).\n",
    "- LogisticRegression(): This is the actual estimator (machine learning model) for the second step. It trains on the scaled data from the previous step.\n",
    "'''\n",
    "\n",
    "# Train\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_predict = pipe.predict(X_test)\n",
    "\n",
    "\n",
    "'''\n",
    "## What Happens Here?\n",
    "\n",
    "- After scaling the data, the pipeline passes the scaled data to the logistic regression model.\n",
    "- The model is trained on the scaled data during .fit() or used to make predictions during .predict().\n",
    "'''\n",
    "\n",
    "'''\n",
    "üåü How Does the Pipeline Work Internally?\n",
    "\n",
    "When we call methods like .fit() or .predict() on the pipeline, here‚Äôs what happens internally:\n",
    "\n",
    "1. During .fit(X_train, y_train)\n",
    "\n",
    "- Step 1: The pipeline applies StandardScaler().fit_transform(X_train) to scale the training data.\n",
    "- Step 2: The pipeline passes the scaled data (i.e. X_train) & y_train to LogisticRegression().fit() to train the model.\n",
    "\n",
    "2. During .predict(X_test)\n",
    "\n",
    "- Step 1: The pipeline applies StandardScaler().transform(X_test) to scale the test data using the same scaler fitted on the training data.\n",
    "- Step 2: The pipeline passes the scaled test data to LogisticRegression().predict() to make predictions.\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
