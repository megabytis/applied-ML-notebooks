# 🔥 Phase 1 — “Minimum Viable ML” (2–3 weeks)

Lock in the core ML intuition with small, real‑world datasets:

- **Linear Regression** ✅

  - Mini-project: Predict the price of new houses using your trained model.

- **Multiple Linear Regression** (size + bedrooms + age → price)

  - Mini-project: Take 5–10 real or imagined houses, predict prices, visualize predicted vs actual prices.

- **Logistic Regression** (classification: pass/fail, spam/ham)

  - Mini-project: Predict whether new students pass/fail based on study hours.

- **Train/Test split + evaluation metrics** (accuracy, precision/recall)

- **Feature encoding** (turn text into numbers)

- **Scaling/normalisation basics**

💡 Output: By the end, we can pick up a small CSV from Kaggle and run a full supervised learning pipeline ourselves.

# ⚡ Phase 2 — “Model Variety” (4–6 weeks)

Once comfortable with the pipeline, expand to common algorithms:

- Decision Trees & Random Forests (loan approval, titanic survival)

- K‑NN (simple classification)

- Gradient Boosting / XGBoost (tabular data powerhouse)

💡 Output: You can try different models, compare them, tune parameters.

# 🚀 Phase 3 — “Deployment Mindset” (ongoing)

- Saving models with joblib
- Creating a tiny Flask/FastAPI endpoint to serve predictions
- Integrating predictions into a React front‑end

💡 Output: A simple web app where someone can input features and see a prediction.
