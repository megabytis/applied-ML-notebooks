# üß† Machine Learning Algorithms Cheatsheet

## ‚úÖ Supervised & Unsupervised Learning ‚Äî All Major Algorithms

---

## üéØ SUPERVISED LEARNING ALGORITHMS

_(Used when you have labeled data ‚Äî input + known output)_

---

### 1Ô∏è‚É£ Regression Algorithms

‚Üí Predict **continuous numerical values**

| Algorithm                                                     | Use Case Example                        | Key Features                                             |
| ------------------------------------------------------------- | --------------------------------------- | -------------------------------------------------------- |
| **Linear Regression**                                         | House price, salary prediction          | Simple, interpretable, assumes linear relationship       |
| **Polynomial Regression**                                     | Non-linear trends (e.g., growth curves) | Fits curved relationships using polynomial terms         |
| **Ridge Regression**                                          | When multicollinearity exists           | Adds L2 regularization to prevent overfitting            |
| **Lasso Regression**                                          | Feature selection + regression          | Adds L1 regularization ‚Äî can shrink coefficients to zero |
| **Elastic Net**                                               | Mix of Ridge + Lasso                    | Good when you have many correlated features              |
| **Support Vector Regression (SVR)**                           | Non-linear regression with margins      | Uses kernels, robust to outliers                         |
| **Decision Tree Regressor**                                   | Predict sales, temperature              | Easy to visualize, handles non-linearity                 |
| **Random Forest Regressor**                                   | Ensemble of trees ‚Äî high accuracy       | Reduces overfitting, handles noise well                  |
| **Gradient Boosting Regressor (XGBoost, LightGBM, CatBoost)** | Winning solution in Kaggle              | Sequential boosting ‚Äî high performance                   |

---

### 2Ô∏è‚É£ Classification Algorithms

‚Üí Predict **discrete categories / labels**

| Algorithm                                                      | Use Case Example                       | Key Features                                                       |
| -------------------------------------------------------------- | -------------------------------------- | ------------------------------------------------------------------ |
| **Logistic Regression**                                        | Spam detection, medical diagnosis      | Probabilistic output, linear decision boundary                     |
| **K-Nearest Neighbors (KNN)**                                  | Image recognition, recommender systems | Simple, instance-based, needs distance metric                      |
| **Naive Bayes**                                                | Text classification, spam filter       | Fast, works well with text, assumes feature independence           |
| **Decision Tree Classifier**                                   | Customer churn, loan approval          | Interpretable, handles mixed data types                            |
| **Random Forest Classifier**                                   | Credit scoring, disease prediction     | Ensemble of trees ‚Äî robust, handles overfitting                    |
| **Support Vector Machine (SVM)**                               | Image classification, bioinformatics   | Powerful for high-dim data, uses kernels for non-linear separation |
| **Gradient Boosting Classifier (XGBoost, LightGBM, CatBoost)** | High-accuracy classification           | Sequential boosting, often wins competitions                       |
| **Neural Networks (MLP)**                                      | Complex pattern recognition            | Flexible, needs lots of data & tuning                              |
| **Stochastic Gradient Descent (SGD) Classifier**               | Large-scale linear classification      | Efficient for big datasets                                         |

---

## üåÄ UNSUPERVISED LEARNING ALGORITHMS

_(Used when you have unlabeled data ‚Äî only input, no output)_

---

### 1Ô∏è‚É£ Clustering Algorithms

‚Üí Group similar data points together

| Algorithm                         | Use Case Example                                | Key Features                                                 |
| --------------------------------- | ----------------------------------------------- | ------------------------------------------------------------ |
| **K-Means Clustering**            | Customer segmentation, image compression        | Simple, fast, needs to choose K (number of clusters)         |
| **Hierarchical Clustering**       | Biology (species grouping), document clustering | Builds tree of clusters (dendrogram), no need to predefine K |
| **DBSCAN**                        | Anomaly detection, spatial data                 | Finds dense regions, handles noise & outliers well           |
| **Gaussian Mixture Models (GMM)** | Soft clustering (probabilistic)                 | Assigns probability of belonging to each cluster             |
| **Mean Shift Clustering**         | Computer vision, tracking objects               | Finds modes (peaks) in data density                          |
| **Affinity Propagation**          | When you don‚Äôt know number of clusters          | Uses message passing, good for small/medium datasets         |

---

### 2Ô∏è‚É£ Dimensionality Reduction Algorithms

‚Üí Reduce number of features while preserving information

| Algorithm                              | Use Case Example                                          | Key Features                                                  |
| -------------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------- |
| **Principal Component Analysis (PCA)** | Visualization, noise reduction                            | Linear, finds directions of max variance                      |
| **t-SNE**                              | Visualizing high-dim data (e.g., images, word embeddings) | Non-linear, great for 2D/3D plots ‚Äî preserves local structure |
| **UMAP**                               | Faster alternative to t-SNE                               | Preserves both local and global structure, scalable           |
| **Linear Discriminant Analysis (LDA)** | Classification + dimensionality reduction                 | Supervised ‚Äî uses class labels to find best projection        |
| **Autoencoders (Neural)**              | Image compression, anomaly detection                      | Deep learning-based, learns efficient encoding                |

---

### 3Ô∏è‚É£ Association Rule Learning

‚Üí Find relationships between variables (e.g., ‚Äúcustomers who buy X also buy Y‚Äù)

| Algorithm             | Use Case Example              | Key Features                                         |
| --------------------- | ----------------------------- | ---------------------------------------------------- |
| **Apriori Algorithm** | Market basket analysis        | Finds frequent itemsets, generates association rules |
| **FP-Growth**         | Faster alternative to Apriori | Uses tree structure, more memory efficient           |

---

### 4Ô∏è‚É£ Anomaly Detection (Often Unsupervised)

‚Üí Find rare or unusual data points

| Algorithm                      | Use Case Example                          | Key Features                           |
| ------------------------------ | ----------------------------------------- | -------------------------------------- |
| **Isolation Forest**           | Fraud detection, system health monitoring | Fast, works well with high-dim data    |
| **One-Class SVM**              | Detect outliers in manufacturing          | Learns boundary around ‚Äúnormal‚Äù data   |
| **Local Outlier Factor (LOF)** | Detect local anomalies                    | Compares density of point to neighbors |

---

## üß≠ Quick Decision Guide

‚úÖ **Want to predict a number?** ‚Üí Use **Regression** algorithms  
‚úÖ **Want to predict a category?** ‚Üí Use **Classification** algorithms  
‚úÖ **Want to group data?** ‚Üí Use **Clustering**  
‚úÖ **Want to reduce features?** ‚Üí Use **Dimensionality Reduction**  
‚úÖ **Want to find buying patterns?** ‚Üí Use **Association Rules**  
‚úÖ **Want to find weird/fraudulent data?** ‚Üí Use **Anomaly Detection**

---

üìå **Pro Tip**:  
Start simple ‚Üí Try Linear/Logistic Regression or K-Means first.  
Then scale up ‚Üí Use ensemble methods (Random Forest, XGBoost) or deep learning if needed.

---

üîñ Save this as `ml_algorithms_cheatsheet.md` ‚Äî perfect for quick revision before interviews, exams, or projects!
