{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05fa20e",
   "metadata": {},
   "source": [
    "# ðŸ” Cross-Validation\n",
    "\n",
    "## âš™ï¸ What is Cross-Validation (CV)?\n",
    "\n",
    "Normally, weâ€™d do this:\n",
    "```py\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "```\n",
    "\n",
    "- But that gives us only one split.\n",
    "- That means our modelâ€™s performance depends heavily on how lucky that split was.\n",
    "- What if the 20% test data was unusually easy or hard? Thatâ€™s biased.\n",
    "- Cross-validation fixes this. ðŸ’¡\n",
    "\n",
    "\n",
    "## ðŸŒŸ Real Problem: The \"Lucky Split\" Trap\n",
    "\n",
    "Imagine we have **10 students** and want to test if our teaching method works.\n",
    "\n",
    "### **Bad Method (Simple Train/Test Split):**\n",
    "- **Pick 2 students randomly** as our \"test group\"\n",
    "- **Teach the other 8 students** our method\n",
    "- **Test only the 2 students** and see how they perform\n",
    "\n",
    "**What could go wrong?**\n",
    "- We might accidentally pick **2 genius students** â†’ 100% success!\n",
    "- Or we might pick **2 struggling students** â†’ 0% success!\n",
    "- Our result depends entirely on **luck of the draw**, not our teaching method!\n",
    "\n",
    "This is exactly what happens with **simple train/test splits** in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Why We Need Cross-Validation\n",
    "\n",
    "### **The Core Problem:**\n",
    "- We have **limited data**\n",
    "- We need to **both train AND test** our model\n",
    "- **Every time we split differently**, we get **different results**\n",
    "- We want to know: **\"How well will my model REALLY perform?\"**\n",
    "\n",
    "### **Cross-Validation Solves This By:**\n",
    "> **\"Let every data point get a chance to be in the test set!\"**\n",
    "\n",
    "Instead of trusting **one lucky split**, we test our model **multiple times** with **different test sets**, then **average the results**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” How Cross-Validation Actually Works\n",
    "\n",
    "### **Step 1: Divide Your Data into K Equal Parts**\n",
    "Let's say we have 10 data points and choose K=5:\n",
    "\n",
    "Our Data: [A, B, C, D, E, F, G, H, I, J]\n",
    "\n",
    "Divided into 5 folds:\n",
    "\n",
    "Fold 1: [A, B]\n",
    "\n",
    "Fold 2: [C, D]\n",
    "\n",
    "Fold 3: [E, F]\n",
    "\n",
    "Fold 4: [G, H]\n",
    "\n",
    "Fold 5: [I, J]\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Train and Test K Times**\n",
    "\n",
    "**Round 1:**\n",
    "- **Train on**: Folds 2,3,4,5 â†’ [C,D,E,F,G,H,I,J]\n",
    "- **Test on**: Fold 1 â†’ [A,B]\n",
    "- **Record accuracy**: 85%\n",
    "\n",
    "**Round 2:**\n",
    "- **Train on**: Folds 1,3,4,5 â†’ [A,B,E,F,G,H,I,J]\n",
    "- **Test on**: Fold 2 â†’ [C,D]\n",
    "- **Record accuracy**: 90%\n",
    "\n",
    "**Round 3:**\n",
    "- **Train on**: Folds 1,2,4,5 â†’ [A,B,C,D,G,H,I,J]\n",
    "- **Test on**: Fold 3 â†’ [E,F]\n",
    "- **Record accuracy**: 80%\n",
    "\n",
    "**Round 4:**\n",
    "- **Train on**: Folds 1,2,3,5 â†’ [A,B,C,D,E,F,I,J]\n",
    "- **Test on**: Fold 4 â†’ [G,H]\n",
    "- **Record accuracy**: 95%\n",
    "\n",
    "**Round 5:**\n",
    "- **Train on**: Folds 1,2,3,4 â†’ [A,B,C,D,E,F,G,H]\n",
    "- **Test on**: Fold 5 â†’ [I,J]\n",
    "- **Record accuracy**: 85%\n",
    "\n",
    "### **Step 3: Calculate Final Performance**\n",
    "\n",
    "Final Accuracy = (85% + 90% + 80% + 95% + 85%) / 5 = 87%\n",
    "\n",
    "\n",
    "\n",
    "**Now you have a much more reliable estimate!**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤” Why This is Better Than Simple Split\n",
    "\n",
    "### **Simple Train/Test Split:**\n",
    "- **Uses only 80% of data for training** (in 80/20 split)\n",
    "- **Tests on only 20% of data**\n",
    "- **Result varies wildly** based on which 20% you pick\n",
    "- **High variance, low confidence**\n",
    "\n",
    "### **5-Fold Cross-Validation:**\n",
    "- **Uses 80% of data for training** in each round\n",
    "- **Tests on 100% of data** (every point gets tested once)\n",
    "- **Averages results** from 5 different test sets\n",
    "- **Lower variance, higher confidence**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  The Magic of Stratified Cross-Validation\n",
    "\n",
    "### **New Problem: Imbalanced Classes**\n",
    "What if your data looks like this?\n",
    "- **90 spam emails**\n",
    "- **10 legitimate emails**\n",
    "\n",
    "### **Regular K-Fold Problem:**\n",
    "- Some folds might have **no legitimate emails**\n",
    "- Model never learns to recognize legitimate emails\n",
    "- Results are **meaningless**\n",
    "\n",
    "### **Stratified K-Fold Solution:**\n",
    "- **Maintains the same class ratio** in every fold\n",
    "- If overall ratio is 90% spam, 10% legit...\n",
    "- **Every fold** also has 90% spam, 10% legit\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "Original Data: 90 spam (S), 10 legit (L)\n",
    "\n",
    "5-Fold Stratified:\n",
    "\n",
    "Fold 1: 18S, 2L\n",
    "\n",
    "Fold 2: 18S, 2L\n",
    "\n",
    "Fold 3: 18S, 2L\n",
    "\n",
    "Fold 4: 18S, 2L\n",
    "\n",
    "Fold 5: 18S, 2L\n",
    "\n",
    "\n",
    "\n",
    "**Every fold has the same balance as your original data!**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ When Should You Use Cross-Validation?\n",
    "\n",
    "### **âœ… Use CV when:**\n",
    "- **Dataset is small** (under 10,000 samples)\n",
    "- **You want reliable performance estimates**\n",
    "- **Comparing different models**\n",
    "- **Tuning hyperparameters**\n",
    "- **Your data is imbalanced** (use Stratified CV)\n",
    "\n",
    "### **âŒ Don't use CV when:**\n",
    "- **Dataset is huge** (takes too long)\n",
    "- **Time series data** (use time-based splits)\n",
    "- **You have a separate validation set** already\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Real-World Example\n",
    "\n",
    "**Scenario**: You built a spam detector with 80% accuracy on your test set.\n",
    "\n",
    "**Without CV**: \"Maybe I just got lucky with my test set split!\"\n",
    "\n",
    "**With 5-Fold CV**: \"My model consistently scores 78-82% across 5 different test sets, averaging 80%.\"\n",
    "\n",
    "**Which result would you trust more?**\n",
    "\n",
    "Exactly! Cross-Validation gives you **confidence** that your model will perform well on **truly new data**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "- **Cross-Validation = Multiple train/test splits averaged together**\n",
    "- **Every data point gets to be in the test set exactly once**\n",
    "- **Provides much more reliable performance estimates**\n",
    "- **Stratified CV maintains class balance in each fold**\n",
    "- **Essential for small datasets and model validation**\n",
    "- **Standard practice in machine learning**\n",
    "\n",
    "> **Remember**: Cross-Validation is like giving your model a **final exam with questions from every chapter**, instead of just one random question! ðŸŽ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05f0d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a340bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0f6224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ade3a6",
   "metadata": {},
   "source": [
    "### Simple K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in each fold: [0.93333333 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "Average accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 5 fold simple cross validation\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print(f\"Accuracy in each fold: {scores}\")\n",
    "print(f\"Average accuracy: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4a933",
   "metadata": {},
   "source": [
    "### Stratified K-Fold (Recommended for imbalanced data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in each fold: [0.93333333 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "Average accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 5 fold startified cross validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "startified_score = cross_val_score(model, X, y, cv=stratified_cv, scoring=\"accuracy\")\n",
    "\n",
    "print(f\"Accuracy in each fold: {scores}\")\n",
    "print(f\"Average accuracy: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111121a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
