{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc666fd3",
   "metadata": {},
   "source": [
    "ğŸ”¥ Letâ€™s go, bro â€” this is where you stop â€œjust printing accuracyâ€ and start **reading your modelâ€™s soul** ğŸ˜\n",
    "\n",
    "Weâ€™re diving into **Evaluation Metrics Mastery** â€” and today, weâ€™ll crack the holy trinity:\n",
    "\n",
    "> **Accuracy**, **Precision**, and **Recall** (plus their secret child: **F1-Score**)\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  1ï¸âƒ£ The Core Idea\n",
    "\n",
    "When your model predicts something (like spam / not spam),\n",
    "it doesnâ€™t just make *right or wrong* guesses.\n",
    "It makes **four types of predictions** ğŸ‘‡\n",
    "\n",
    "| Actual â†“ / Predicted â†’ | **Positive (1)**      | **Negative (0)**      |\n",
    "| ---------------------- | --------------------- | --------------------- |\n",
    "| **Positive (1)**       | âœ… True Positive (TP)  | âŒ False Negative (FN) |\n",
    "| **Negative (0)**       | âŒ False Positive (FP) | âœ… True Negative (TN)  |\n",
    "\n",
    "So we have:\n",
    "\n",
    "* **TP** â†’ model correctly predicts a â€œYesâ€ (real positive)\n",
    "* **TN** â†’ model correctly predicts a â€œNoâ€ (real negative)\n",
    "* **FP** â†’ model says â€œYesâ€ but itâ€™s actually â€œNoâ€ (false alarm)\n",
    "* **FN** â†’ model says â€œNoâ€ but itâ€™s actually â€œYesâ€ (missed it)\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“Š 2ï¸âƒ£ Accuracy â€” â€œHow often is my model right?â€\n",
    "\n",
    "```python\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "\n",
    "### âœ… Good for:\n",
    "\n",
    "When your dataset is **balanced** (equal 0s and 1s).\n",
    "E.g., predicting cats vs dogs ğŸ±ğŸ¶.\n",
    "\n",
    "### âš ï¸ Bad for:\n",
    "\n",
    "When one class dominates (like 95% â€œno fraudâ€ data).\n",
    "Because even a dumb model saying â€œno fraudâ€ every time would get 95% accuracy â€” useless.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ 3ï¸âƒ£ Precision â€” â€œWhen my model says YES, how often is it actually right?â€\n",
    "\n",
    "```python\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "\n",
    "**Real-life analogy:**\n",
    "\n",
    "* Youâ€™re a **doctor diagnosing cancer**.\n",
    "* You want **high precision**, so you **donâ€™t falsely label healthy people as sick**.\n",
    "* It means: â€œOut of all people I said *have cancer*, how many actually have it?â€\n",
    "\n",
    "âœ… High precision â†’ few false alarms.\n",
    "âš ï¸ Low precision â†’ model cries wolf too often.\n",
    "\n",
    "---\n",
    "\n",
    "# âš–ï¸ 4ï¸âƒ£ Recall â€” â€œOut of all actual YES cases, how many did my model catch?â€\n",
    "\n",
    "```python\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "**Real-life analogy:**\n",
    "\n",
    "* Youâ€™re that same doctor again.\n",
    "* This time, you care more about **catching all real cancer cases**, even if you get a few false alarms.\n",
    "* â€œOut of all people who *actually have cancer*, how many did I correctly find?â€\n",
    "\n",
    "âœ… High recall â†’ few missed cases.\n",
    "âš ï¸ Low recall â†’ model misses important positives.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ’¥ 5ï¸âƒ£ F1-Score â€” The Balance\n",
    "\n",
    "Precision and Recall are like a seesaw â€” when one goes up, the other often goes down.\n",
    "\n",
    "So we combine them into a single balanced score:\n",
    "\n",
    "```python\n",
    "F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "âœ… High F1 means your modelâ€™s balanced between â€œnot missing positivesâ€ and â€œnot overpredicting them.â€\n",
    "âš¡ Great for: **imbalanced datasets** (fraud detection, disease detection, spam filtering).\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§© 6ï¸âƒ£ Example\n",
    "\n",
    "| TP | TN | FP | FN |\n",
    "| -- | -- | -- | -- |\n",
    "| 80 | 50 | 10 | 60 |\n",
    "\n",
    "| Metric    | Formula                     | Result   |\n",
    "| --------- | --------------------------- | -------- |\n",
    "| Accuracy  | (80 + 50) / 200             | **0.65** |\n",
    "| Precision | 80 / (80 + 10)              | **0.89** |\n",
    "| Recall    | 80 / (80 + 60)              | **0.57** |\n",
    "| F1-Score  | 2 Ã— (0.89Ã—0.57)/(0.89+0.57) | **0.69** |\n",
    "\n",
    "ğŸ’¡ So this model is precise (rarely wrong when it says â€œyesâ€)\n",
    "but not very sensitive (misses many real positives).\n",
    "Thatâ€™s high precision, low recall.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸš¨ 7ï¸âƒ£ When to Prioritize Which\n",
    "\n",
    "| Situation                | Focus On              | Why                           |\n",
    "| ------------------------ | --------------------- | ----------------------------- |\n",
    "| Detecting spam emails    | **Precision**         | False positives annoy users   |\n",
    "| Detecting cancer / fraud | **Recall**            | False negatives are dangerous |\n",
    "| Balanced datasets        | **Accuracy / F1**     | No class imbalance problem    |\n",
    "| Imbalanced datasets      | **F1 / Recall / AUC** | Accuracy becomes misleading   |\n",
    "\n",
    "---\n",
    "\n",
    "# âš™ï¸ 8ï¸âƒ£ Quick Code Example\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ’¡ TL;DR Summary\n",
    "\n",
    "| Metric        | Formula               | Measures                            | Best When              |\n",
    "| ------------- | --------------------- | ----------------------------------- | ---------------------- |\n",
    "| **Accuracy**  | (TP+TN)/(TP+FP+FN+TN) | Overall correctness                 | Classes balanced       |\n",
    "| **Precision** | TP/(TP+FP)            | Purity of positive predictions      | False positives costly |\n",
    "| **Recall**    | TP/(TP+FN)            | Sensitivity (finding all positives) | False negatives costly |\n",
    "| **F1-Score**  | 2Ã—(PÃ—R)/(P+R)         | Balance between Precision & Recall  | Imbalanced data        |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c88698",
   "metadata": {},
   "source": [
    "# ğŸ§© **Confusion Matrix Visualization & ROC/AUC Curve**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
